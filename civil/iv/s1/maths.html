<!DOCTYPE html>
<html>
  <head>
    <title>Maths (EM-41007)</title>
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta charset="UTF-8" />
    <link rel="stylesheet" href="../style.css" />
    <script src="../../mathjax/es5/tex-mml-chtml.js"></script>
    </head>
  <body>
<div class="top">Maths (EM-41007)</div>
<div class="container">

<p><span>CASIO fx-991EX</span><br>$$\displaylines{ \textsf{Fix}/\textsf{Sci}\Rightarrow \textsf{SETUP}\rightarrow 3\\\\\textsf{Interpolation}\Rightarrow\\\textsf{MENU}\rightarrow 6\rightarrow 2/3\rightarrow\textsf{AC}\rightarrow\\...\rightarrow\textsf{OPTN}\rightarrow\;\downarrow\;\rightarrow 4\rightarrow 5/6\;(\hat{y})\\\\\textsf{Matrix}\Rightarrow\textsf{MENU}\rightarrow 4\\\textsf{Then, OPTN to define/edit/calculate Matrix} }$$</p>

<h3>Chapter - 1</h3>

<h3>19.2 (Solution of Equations by Iteration)</h3>
<p><span>I. Fixed-Point Iteration Method</span><br>$$\displaylines{
f(x)=0\quad\quad x=g(x)\\
 |\,g'(x)\,|\lt 1\;\Rightarrow\; \textsf{Convergence}\\|\,g'(x)\,|\gt 1\;\Rightarrow\;\textsf{Divergence}\\x_{n+1}=g(x_n)\,,\;n=0,1,2,...\\\textsf{Error = Exact - Approximation} }$$</p>
<p><span>II. Newton's Method</span><br>$$\displaylines{ x_{n+1}=x_n-\frac{f(x_n)}{f'(x_n)}\,,\; n=0,1,2,... }$$</p>
<p><span>III. Secant Method</span><br>$$\displaylines{ x_{n+1}=x_n-f(x_n)\,\frac{x_n-x_{n-1}}{f(x_n)-f(x_{n-1})}\\n=1,2,3,...\\\\A=x_0\quad B=x_1\quad C=x_2\\C=B-\frac{(...)(B-A)}{(...)-(...)}:A=B:B=C }$$</p>
<p><span>IV. Bisection Method</span><br>$$\displaylines{ f(x_l)f(x_u)\lt 0\\x_m=\frac{x_l+x_u}{2}\\x=\frac{A+B}{2}:f(x)\\\\f(x_l)f(x_m)\lt 0\\\therefore\textsf{The root lies in the lower subinterval.}\\\therefore x_m=x_u\\\\f(x_l)f(x_m)\gt 0\\\therefore\textsf{The root lies in the upper subinterval.}\\\therefore x_m=x_l\\\\f(x_l)f(x_m)=0\\\therefore\textsf{The root equals \(x_m\).} }$$</p>
<p><span>V. Method of False Position</span><br>$$\displaylines{ c_n=\frac{a_nf(b_n)-b_nf(a_n)}{f(b_n)-f(a_n)}\,,\; n=0,1,2,...\\\\A=a_0\quad B=b_0\\C=f(a_0)\quad D=f(b_0)\\x=\frac{AD-BC}{D-C}:f(x)\\\\\textsf{Choose}\;a_0\,,\;b_0\Rightarrow f(a_0)f(b_0)\lt 0\\f(a_n)f(c_n)\lt 0\Rightarrow b_n=c_n\\f(a_n)f(c_n)\gt 0\Rightarrow a_n=c_n }$$</p>

<h3>19.3 (Interpolation)</h3>
<p><span>Linear Lagrange Interpolation</span><br>$$\displaylines{ \begin{align}f(x)\approx p_1(x)&=L_0(x)f_0+L_1(x)f_1\\&=\frac{x-x_1}{x_0-x_1}\;f_0+\frac{x-x_0}{x_1-x_0}\;f_1\end{align}\\\textsf{Error = Exact - Approximation} }$$</p>
<p><span>Quadratic Lagrange Interpolation</span><br>$$\displaylines{ \begin{align}f(x)\approx p_2(x)&=L_0(x)f_0+L_1(x)f_1+L_2(x)f_2\\&=\frac{(x-x_1)(x-x_2)}{(x_0-x_1)(x_0-x_2)}\;f_0\\&+\frac{(x-x_0)(x-x_2)}{(x_1-x_0)(x_1-x_2)}\;f_1\\&+\frac{(x-x_0)(x-x_1)}{(x_2-x_0)(x_2-x_1)}\;f_2\end{align} }$$</p>
<p><span>Error Estimate</span><br>$$\displaylines{ \varepsilon_n(x)=(x-x_0)(x-x_1)...(x-x_n)\\\times\frac{f^{(n+1)}(t)}{(n+1)!}\\\\\varepsilon_1(x)=(x-x_0)(x-x_1)\;\frac{f''(t)}{2!} }$$</p>
<p><span>Newton's Divided Difference Interpolation</span><br>$$\displaylines{ f(x)\approx f_0+(x-x_0)f[x_0\,, x_1]\\+(x-x_0)(x-x_1)f[x_0\,, x_1\,, x_2]\\+(x-x_0)(x-x_1)(x-x_2)f[x_0\,, x_1\,, x_2\,, x_3]\\+...+(x-x_0)(x-x_1)...(x-x_{n-1})\\\times f[x_0\,, ...\,, x_n]\\\\
 1^{\textsf{st}}\textsf{ order divided difference}\\
 f[x_0\,, x_1]=\frac{f_1-f_0}{x_1-x_0}\\
 f[x_1\,, x_2]=\frac{f_2-f_1}{x_2-x_1}\\
 f[x_2\,, x_3]=\frac{f_3-f_2}{x_3-x_2}\\\\
 2^{\textsf{nd}}\textsf{ order divided difference}\\
 f[x_0\,, x_1\,, x_2]=\frac{f[x_1\,, x_2]-f[x_0\,, x_1]}{x_2-x_0}\\
 f[x_1\,, x_2\,, x_3]=\frac{f[x_2\,, x_3]-f[x_1\,, x_2]}{x_3-x_1}\\\\
 3^{\textsf{rd}}\textsf{ order divided difference}\\
 f[x_0\,, x_1\,, x_2\,, x_3]=\\\frac{f[x_1\,, x_2\,, x_3]-f[x_0\,, x_1\,, x_2]}{x_3-x_0}
}$$</p>
<p><span>Newton's Forward Difference Formula</span></br>$$\displaylines{ f(x)\approx p_n(x)=f_0+r\Delta f_0+ \frac{r(r-1)}{2!}\Delta^2f_0\\+\frac{r(r-1)(r-2)}{3!}\Delta^3f_0+...\\\\
r=\frac{x-x_0}{h}\\\\
\Delta^3f_j=\Delta^2f_{j+1}-\Delta^2f_j\\
\Delta^2f_j=\Delta f_{j+1}-\Delta f_j\\
\Delta f_j=f_{j+1}-f_j\\\\
\textsf{Error Estimate}\\
\begin{align}
\varepsilon_n(x)&=f(x)-p_n(x)\\&=\frac{h^{n+1}}{(n+1)!}r(r-1)...(r-n)f^{n+1}(t)
\end{align}\\\\
f(x)=\varepsilon_n(x)+p_n(x)
 }$$</p>
<p><span>Newton's Backward Difference Formula</span></br>$$\displaylines{ f(x)\approx p_n(x)=f_0+r\nabla f_0+ \frac{r(r+1)}{2!}\nabla^2f_0\\+\frac{r(r+1)(r+2)}{3!}\nabla^3f_0+...\\\\
r=\frac{x-x_0}{h}\\\\
\nabla^3f_j=\nabla^2f_j-\nabla^2f_{j-1}\\
\nabla^2f_j=\nabla f_j-\nabla f_{j-1}\\
\nabla f_j=f_j-f_{j-1}
}$$</p>

<h3>19.5 (Numeric Integration and Differentiation)</h3>

<p><span>Rectangular Rule</span><br>$$\displaylines{ h=\frac{b-a}{n}\quad x_1^*=\frac{x_0+x_1}{2}\quad x_2^*=\frac{x_1+x_2}{2}\\\\
J=\int_a^b f(x)\,dx\\\approx h[\;f(x_1^*)+f(x_2^*)+f(x_3^*)+...+f(x_n^*)\;]}$$</p>
<p><span>Trapezoidal Rule</span><br>$$\displaylines{ h=\frac{b-a}{n}\\\\
J_h=\int_a^bf(x)\,dx\\\approx h[\;\frac{1}{2}(f_0+f_n)+(f_1+f_2+...+f_{n-1})\;]\\\\
\textsf{Error Bounds}\\
KM_2\le\varepsilon\le KM_2^*\quad\quad K=\frac{-(b-a)^3}{12n^2}\\
M_2=f''(b)\quad\quad M_2^*=f''(a)\\\\
\textsf{Error Estimate}\\
\varepsilon_{\frac{h}{2}} \approx \frac{1}{3}(J_{\frac{h}{2}}-J_h)\\\\
J_{\frac{h}{2}}\approx \frac{h}{2}[\;\frac{1}{2}(f_0+f_n)\\+(f_1+f_2+...+f_{n-1})\;]
 }$$</p>
<p><span>Simpson's Rule</span><br>$$\displaylines{ J=\int_a^b f(x)\,dx\approx \frac{h}{3}[\,(f_0+f_{2m})\\+\,4(f_1+f_3\,+...+f_{2m-1})\\+\,2(f_2+f_4\,+...+f_{2m-2})\,]\\\\h=\frac{b-a}{2m} }$$</p>

<h3>Chapter - 2</h3>

<h3>Numeric Linear Algebra</h3>

<h3>20.1 (Linear System)</h3>
 
<p><span>Matrix</span><br>$$\displaylines{ 
\begin{bmatrix}
a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33}
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2 \\
x_3
\end{bmatrix}=
\begin{bmatrix}
b_1 \\
b_2 \\
b_3
\end{bmatrix}\\A\,\underline{x}=\underline{b}\\
a_{11}x_1+a_{12}x_2+a_{13}x_3=b_1\\
a_{21}x_1+a_{22}x_2+a_{23}x_3=b_2\\
a_{31}x_1+a_{32}x_2+a_{33}x_3=b_3
 }$$</p>
<p><span>Gauss Elimination</span><br>$$\displaylines{ \begin{align}&\Rightarrow x_1\textsf{ in eqn 1.}\\&\Rightarrow\textsf{Arrange eqn in descending order.}\\&\Rightarrow\textsf{Elimination}\\&\Rightarrow\textsf{Convert to upper triangular matrix.}\\
&\quad\begin{bmatrix}
* & * & * & ┇ & *\\
0 & * & * & ┇ & *\\
0 & 0 & * & ┇ & *
\end{bmatrix}\\\\
&\Rightarrow\textsf{Back substitution}
\\\\
&\textsf{I. Unique solution}\Rightarrow \underline{x}=\begin{bmatrix}
b_1 \\
b_2 \\
b_3
\end{bmatrix}\\
&\textsf{II. No solution}\Rightarrow 0x_3=b\\
&\textsf{III. Infinitely many solutions}\Rightarrow 0x_3=0
\end{align}
 }$$</p>
 
<h3>20.2 (LU-Factorization, Matrix Inversion)</h3>

<p><span>I. Doolittle's Method</span><br>$$\displaylines{ 
\textsf{\(x_1\) in equation 1}\\\\
\begin{align}
&(i)\; A\underline{x}=\underline{b}\\
&(ii)\; A=LU \quad L=
\begin{bmatrix}
1 & 0 & 0\\
a & 1 & 0\\
b & c & 1
\end{bmatrix}\\\\
&\quad\quad\quad\quad\quad\quad\: U=
\begin{bmatrix}
d & e & f\\
0 & g & h\\
0 & 0 & i
\end{bmatrix}\\\\
&(iii)\; L\underline{y}=\underline{b}\quad\Rightarrow \underline{y}=?\Rightarrow \underline{y}=
\begin{bmatrix}
y_1\\
y_2\\
y_3
\end{bmatrix}\\\\
&(iv)\; U\underline{x}=\underline{y}\quad\Rightarrow \underline{x}=?\Rightarrow \underline{x}=
\begin{bmatrix}
x_1\\
x_2\\
x_3
\end{bmatrix}\\\\
&\textsf{L = Lower triangular matrix (upper zero)}\\
&\textsf{U = Upper triangular matrix (lower zero)}
\end{align}
 }$$</p>
<p><span>II. Crout's Method</span><br>$$\displaylines{ 
\begin{align}
&(i)\; A\underline{x}=\underline{b}\\
&(ii)\; A=LU \quad L=
\begin{bmatrix}
a & 0 & 0\\
b & c & 0\\
d & e & f
\end{bmatrix}\\\\
&\quad\quad\quad\quad\quad\quad\: U=
\begin{bmatrix}
1 & g & h\\
0 & 1 & i\\
0 & 0 & 1
\end{bmatrix}\\\\
&(iii)\; L\underline{y}=\underline{b}\quad\Rightarrow \underline{y}=?\Rightarrow \underline{y}=
\begin{bmatrix}
y_1\\
y_2\\
y_3
\end{bmatrix}\\\\
&(iv)\; U\underline{x}=\underline{y}\quad\Rightarrow \underline{x}=?\Rightarrow \underline{x}=
\begin{bmatrix}
x_1\\
x_2\\
x_3
\end{bmatrix}\\\\
\end{align}
 }$$</p>
<p><span>III. Cholesky's Method</span><br>$$\displaylines{ 
\begin{align}
&(i)\; A\underline{x}=\underline{b}\\
&\quad\; A=A^T\Rightarrow\textsf{A is symmetric matrix}\\\\
&(ii)\; A=LU=LL^T\quad\; (U=L^T)\\
&\quad\; A=
\begin{bmatrix}
a & 0 & 0\\
b & c & 0\\
d & e & f
\end{bmatrix}
\begin{bmatrix}
a & b & d\\
0 & c & e\\
0 & 0 & f
\end{bmatrix}\\\\
&(iii)\; L\underline{y}=\underline{b}\quad\Rightarrow \underline{y}=?\Rightarrow \underline{y}=
\begin{bmatrix}
y_1\\
y_2\\
y_3
\end{bmatrix}\\\\
&(iv)\; U\underline{x}=\underline{y}\\
&\quad\; L^T\underline{x}=\underline{y}\quad\Rightarrow \underline{x}=?\Rightarrow \underline{x}=
\begin{bmatrix}
x_1\\
x_2\\
x_3
\end{bmatrix}\\\\
\end{align}
 }$$</p>
<p><span>Matrix Inversion (Gauss-Jordan Method)</span><br>$$\displaylines{ 
a_{jj}\neq 0\\\\
\begin{align}
&(i)\quad A=
\begin{bmatrix}\quad
\end{bmatrix}\textsf{ (or) Linear system equations}\\
&(ii)\;\;
\left[\;
\begin{array}{c|c}
A & I\\
\end{array}
\;\right]\quad\Rightarrow\quad
\left[\;
\begin{array}{c|c}
I & A^{-1}\\
\end{array}
\;\right]\\\\
&\quad\quad I = \begin{bmatrix}1&0&0\\0&1&0\\0&0&1\end{bmatrix}\\\\
&\quad\quad A\underline{x}=\underline{b}\\
&\quad\quad \underline{x}=A^{-1}\underline{b}
\end{align}
 }$$</p>

<h3>20.3 (Solution by Iteration)</h3>

<p><span>I. Gauss-Seidel Iteration Method</span><br>$$\displaylines{ 
a_{11}x_1+a_{12}x_2+a_{13}x_3=b_1\\
a_{21}x_1+a_{22}x_2+a_{23}x_3=b_2\\
a_{31}x_1+a_{32}x_2+a_{33}x_3=b_3\\
|a_{11}|\gt |a_{12}|+|a_{13}|\Rightarrow \textsf{eqn(1)}\Rightarrow x_1=?\\
|a_{22}|\gt |a_{21}|+|a_{23}|\Rightarrow \textsf{eqn(2)}\Rightarrow x_2=?\\
|a_{33}|\gt |a_{31}|+|a_{32}|\Rightarrow \textsf{eqn(3)}\Rightarrow x_3=?\\
\\\textsf{* Main diagonal entries must be largest. *}\\
 }$$</p>
<p><span>II. Convergence and Matrix Norms</span><br>
$$\displaylines{ 
a_{jj}=1\\
A=I+L+U\\\\
\begin{bmatrix}
1 & * & *\\
* & 1 & *\\
* & * & 1
\end{bmatrix}=
\begin{bmatrix}
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1
\end{bmatrix}+
\begin{bmatrix}
0 & 0 & 0\\
* & 0 & 0\\
* & * & 0
\end{bmatrix}\\+
\begin{bmatrix}
0 & * & *\\
0 & 0 & *\\
0 & 0 & 0
\end{bmatrix}
\\\\C=-(I+L)^{-1}U\\||C||=\sqrt{\sum_{j=1}^n\;\sum_{k=1}^n\;C_{jk}^2}\quad\textsf{(Frobenius Norm)}\\\\||C||\lt 1\\\therefore\textsf{Gauss-Seidel iteration converges.}
 }$$</p>
<p><span>III. Jacobi Iteration</span><br>$$\displaylines{ x^{(m+1)}=\underline{b}+(I-A)\,x^{(m)}\quad\quad a_{jj}=1 }$$</p>

<h3>20.5 (Least Squares Method)</h3>

<p><span>I. Straight Line</span><br>$$\displaylines{ 
\textsf{Given points: }(x_1,\;y_1),\;(x_2,\;y_2),...,\;(x_n,\;y_n)\\
y=a+bx\\\\
\textsf{Normal equations are}\\
n\,a+b\sum_{j=1}^nx_j=\sum_{j=1}^ny_j\\
a\sum_{j=1}^nx_j+b\sum_{j=1}^nx_j^2=\sum_{j=1}^nx_j\,y_j
 }$$</p>
<p><span>II. Parabola</span><br>$$\displaylines{ 
\textsf{Given points: }(x_1,\;y_1),\;(x_2,\;y_2),...,\;(x_n,\;y_n)\\
y=b_0+b_1x+b_2x^2\\\\
\textsf{Normal equations are}\\
n\,b_0+b_1\sum_{j=1}^nx_j+b_2\sum_{j=1}^nx_j^2=\sum_{j=1}^ny_j\\
b_0\sum_{j=1}^nx_j+b_1\sum_{j=1}^nx_j^2+b_2\sum_{j=1}^nx_j^3=\sum_{j=1}^nx_j\,y_j\\
b_0\sum_{j=1}^nx_j^2+b_1\sum_{j=1}^nx_j^3+b_2\sum_{j=1}^nx_j^4=\sum_{j=1}^nx_j^2\,y_j
 }$$</p>

<h3>Chapter - 3</h3>

<h3>Numeric of ODEs</h3>

<h3>21.1 (Methods for First-Order ODEs)</h3>

<p><span>I. Euler Method</span><br>$$\displaylines{ y'=f(x,\;y)\quad\quad y(x_0)=y_0\\y_{n+1}=y_n+hf(x_n,\;y_n)\,,n=0,\,1,\,2,\,...\\x_{n+1}=x_n+h\\\textsf{Error}=\textsf{Exact}-\textsf{Approximation}\\\\y'=x+y\\y'-y=x\\\textsf{Compare with }y'+f(x)\,y=r(x)\\h=\int f(x)\,dx\\y=e^{-h}\,[\,\int e^h.r(x)\,dx+c\,]\\\\A=\textsf{Exact}:B=y_{n+1}\\:C=A-y:x=x+h:y=B }$$</p>
<p><span>II. Improved Euler Method</span><br>$$\displaylines{ y_{n+1}^*=y_n+hf(x_n,\;y_n)\\\\y_{n+1}=y_n+\frac{h}{2}\,[\,f(x_n,\,y_n)+f(x_{n+1},\,y_{n+1}^*)\,]\\n=0,\,1,\,2,\,... }$$</p>
<p><span>III. Runge-Kutta Method</span><br>$$\displaylines{ y_{n+1}=y_n+\frac{1}{6}\,(k_1+2k_2+2k_3+k_4)\\n=0,\,1,\,2,\,...\\\\k_1=hf(x_n,\,y_n)\\\\k_2=hf(x_n+\frac{h}{2},\,y_n+\frac{k_1}{2})\\\\k_3=hf(x_n+\frac{h}{2},\,y_n+\frac{k_2}{2})\\\\k_4=hf(x_n+h,\,y_n+k_3) }$$</p>
<p><span>IV. Backward Euler Method</span><br>$$\displaylines{ y_{n+1}=y_n+hf(x_{n+1},\,y_{n+1}) }$$</p>

<h3>21.2 (Multistep Methods)</h3>

<p><span>I. Adams-Bashforth Methods</span><br>$$\displaylines{ 
y_1,\,y_2,\,y_3\,\textsf{ given}\\
y_{n+1}=y_n+\frac{h}{24}\,(55f_n-59f_{n-1}+37f_{n-2}\\-9f_{n-3}) }$$</p>
<p><span>II. Adams-Moulton Methods</span><br>$$\displaylines{ 
y_1,\,y_2,\,y_3\,\textsf{ given}\\
y_{n+1}^*=y_n+\frac{h}{24}\,(55f_n-59f_{n-1}+37f_{n-2}\\-9f_{n-3})\\\\y_{n+1}=y_n+\frac{h}{24}\,(9f_{n+1}^*+19f_n-5f_{n-1}\\+f_{n-2})\\\\f_n=f(x_n,\,y_n)\\f_{n+1}^*=f(x_{n+1},\,y_{n+1}^*)\\f_{n-1}=f(x_{n-1},\,y_{n-1})\\f_{n-2}=f(x_{n-2},\,y_{n-2})\\f_{n-3}=f(x_{n-3},\,y_{n-3}) }$$</p>

<h3>Chapter - 4</h3>

<h3>Unconstrained Optimization</h3>

<h3>Linear Programming</h3>

<h3>22.1 (Method of Steepest Descent)</h3>

<p><span>Notes</span><br>$$\displaylines{ \textsf{For two dimensional,}\\\underline{\nabla}\,f(x)=\frac{\partial f}{\partial x_1}\,\hat{i}+\frac{\partial f}{\partial x_2}\,\hat{j}\\\\\textsf{For three dimensional,}\\\underline{\nabla}\,f(x)=\frac{\partial f}{\partial x_1}\,\hat{i}+\frac{\partial f}{\partial x_2}\,\hat{j}+\frac{\partial f}{\partial x_3}\,\hat{k}\\\\z(t)=\underline{x}-t\,\underline{\nabla}\,f(x)\\\underline{x}=x_1\,\hat{i}+x_2\,\hat{j}\\g(t)=f(z(t))\\g'(t)=0\Rightarrow t=? }$$</p>

<h3>22.2 (Linear Programming)</h3>

<p><span>Graphical Method</span><br>$$\displaylines{ \textsf{Find \(x_1\) and \(x_2\) for sketching straight lines.}\\\textsf{Minimize}\Rightarrow\textsf{Smallest value of f}\\\textsf{Maximize}\Rightarrow\textsf{Largest value of f}\\\textsf{Sketch direction can be checked by}\\\textsf{substituting }x_1\textsf{ and }x_2\textsf{ in constraints.} }$$</p>

<h3>22.3 (Simplex Method)</h3>

<p><span>Simplex Tableau</span><br>$$\displaylines{ 
T_0=
\begin{array}{c}
z\quad\quad\;\; x_1\quad\;\; x_2\quad\quad\; x_3\quad\;\; x_4\quad\quad\;\; b \\ \hline
\begin{bmatrix}
\begin{array}{ccccccccc}
  1 &┇& *\quad & * &┇& *\quad & * &┇& * \\ 
  0 &┇& *\quad & * &┇& *\quad & * &┇& * \\ 
  0 &┇& *\quad & * &┇& *\quad & * &┇& * \\ 
\end{array}
\end{bmatrix}
\end{array}
 }$$</p>
<p><span>Notes</span><br>$$\displaylines{ \textsf{Use slack variables for normal form.}\\
\textsf{Minimize}\Rightarrow\textsf{Only 0 and \(-\) in z=1 line}\\
\textsf{Maximize}\Rightarrow\textsf{Only 0 and + in z=1 line}\\
\textsf{Select pivot column and find b/x ratios.}\\
\textsf{Choose pivot row that have}\\\textsf{smallest positive ratio of b/x.}
 }$$</p>

<h3>22.4 (Simplex Method: Difficulties)</h3>

<p><span>Notes</span><br>$$\displaylines{ \le\quad\Rightarrow\quad +x\\\ge\quad\Rightarrow\quad -x
 }$$</p><br>

</div>
<script>
	document.querySelectorAll(".container span, .container h3").forEach(a => a.className = 'maths');
</script>
<script src="mathjax/es5/tex-mml-chtml.js"></script>
    </body>
</html>